<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>ClickHouse稀疏索引原理解读 | Wenming's Blog</title>
<meta name=keywords content="ClickHouse"><meta name=description content="问个问题，如何优化一条SQL语句？我们首先想到的肯定是建索引。对于ClickHouse也不例外，尤其是稀疏主键索引（类似传统数据库中的主键索引）对性能的影响非常大。在下文中，我将结合例子对稀疏主键索引进行详细解读。
注：本文内容主要参考官方文档，如果有余力，强烈建议先行阅读
数据准备 这里，我将就我比较熟悉的时序数据进行举例。首先通过如下SQL建表：
-- create dabase create database test; use test; -- create table CREATE TABLE cpu ( `hostname` String, `reginon` String, `datacenter` String, `timestamp` DateTime64(9,'Asia/Shanghai') CODEC (DoubleDelta), `usage_user` Float64, `usage_system` Float64 ) ENGINE = MergeTree() PRIMARY KEY tuple(); optimize table cpu final ; 并将本地的样本数据导入：
cat example/output.csv |clickhouse-client -d test -q 'INSERT into cpu FORMAT CSV' output.csv是时序数据样本，时间间隔为1秒，包含了从2022-01-01 08:00:00到2022-01-15 07:59:59一共1209600条记录
optimize table 会强制进行merge之类的操作，使其达到最终状态
查询某段时间范围内的CPU使用率 SQL如下:
select ts, avg(usage_user) from cpu where timestamp > '2022-01-15 06:59:59."><meta name=author content="wmingj"><link rel=canonical href=https://wmingj.github.io/posts/clickhouse-sparse-index/><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=https://wmingj.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://wmingj.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://wmingj.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://wmingj.github.io/apple-touch-icon.png><link rel=mask-icon href=https://wmingj.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://wmingj.github.io/posts/clickhouse-sparse-index/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="ClickHouse稀疏索引原理解读"><meta property="og:description" content="问个问题，如何优化一条SQL语句？我们首先想到的肯定是建索引。对于ClickHouse也不例外，尤其是稀疏主键索引（类似传统数据库中的主键索引）对性能的影响非常大。在下文中，我将结合例子对稀疏主键索引进行详细解读。
注：本文内容主要参考官方文档，如果有余力，强烈建议先行阅读
数据准备 这里，我将就我比较熟悉的时序数据进行举例。首先通过如下SQL建表：
-- create dabase create database test; use test; -- create table CREATE TABLE cpu ( `hostname` String, `reginon` String, `datacenter` String, `timestamp` DateTime64(9,'Asia/Shanghai') CODEC (DoubleDelta), `usage_user` Float64, `usage_system` Float64 ) ENGINE = MergeTree() PRIMARY KEY tuple(); optimize table cpu final ; 并将本地的样本数据导入：
cat example/output.csv |clickhouse-client -d test -q 'INSERT into cpu FORMAT CSV' output.csv是时序数据样本，时间间隔为1秒，包含了从2022-01-01 08:00:00到2022-01-15 07:59:59一共1209600条记录
optimize table 会强制进行merge之类的操作，使其达到最终状态
查询某段时间范围内的CPU使用率 SQL如下:
select ts, avg(usage_user) from cpu where timestamp > '2022-01-15 06:59:59."><meta property="og:type" content="article"><meta property="og:url" content="https://wmingj.github.io/posts/clickhouse-sparse-index/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-08-13T16:23:14+08:00"><meta property="article:modified_time" content="2022-08-13T16:23:14+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="ClickHouse稀疏索引原理解读"><meta name=twitter:description content="问个问题，如何优化一条SQL语句？我们首先想到的肯定是建索引。对于ClickHouse也不例外，尤其是稀疏主键索引（类似传统数据库中的主键索引）对性能的影响非常大。在下文中，我将结合例子对稀疏主键索引进行详细解读。
注：本文内容主要参考官方文档，如果有余力，强烈建议先行阅读
数据准备 这里，我将就我比较熟悉的时序数据进行举例。首先通过如下SQL建表：
-- create dabase create database test; use test; -- create table CREATE TABLE cpu ( `hostname` String, `reginon` String, `datacenter` String, `timestamp` DateTime64(9,'Asia/Shanghai') CODEC (DoubleDelta), `usage_user` Float64, `usage_system` Float64 ) ENGINE = MergeTree() PRIMARY KEY tuple(); optimize table cpu final ; 并将本地的样本数据导入：
cat example/output.csv |clickhouse-client -d test -q 'INSERT into cpu FORMAT CSV' output.csv是时序数据样本，时间间隔为1秒，包含了从2022-01-01 08:00:00到2022-01-15 07:59:59一共1209600条记录
optimize table 会强制进行merge之类的操作，使其达到最终状态
查询某段时间范围内的CPU使用率 SQL如下:
select ts, avg(usage_user) from cpu where timestamp > '2022-01-15 06:59:59."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://wmingj.github.io/posts/"},{"@type":"ListItem","position":2,"name":"ClickHouse稀疏索引原理解读","item":"https://wmingj.github.io/posts/clickhouse-sparse-index/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"ClickHouse稀疏索引原理解读","name":"ClickHouse稀疏索引原理解读","description":"问个问题，如何优化一条SQL语句？我们首先想到的肯定是建索引。对于ClickHouse也不例外，尤其是稀疏主键索引（类似传统数据库中的主键索引）对性能的影响非常大。在下文中，我将结合例子对稀疏主键索引进行详细解读。\n注：本文内容主要参考官方文档，如果有余力，强烈建议先行阅读\n数据准备 这里，我将就我比较熟悉的时序数据进行举例。首先通过如下SQL建表：\n-- create dabase create database test; use test; -- create table CREATE TABLE cpu ( `hostname` String, `reginon` String, `datacenter` String, `timestamp` DateTime64(9,\u0026#39;Asia/Shanghai\u0026#39;) CODEC (DoubleDelta), `usage_user` Float64, `usage_system` Float64 ) ENGINE = MergeTree() PRIMARY KEY tuple(); optimize table cpu final ; 并将本地的样本数据导入：\ncat example/output.csv |clickhouse-client -d test -q \u0026#39;INSERT into cpu FORMAT CSV\u0026#39; output.csv是时序数据样本，时间间隔为1秒，包含了从2022-01-01 08:00:00到2022-01-15 07:59:59一共1209600条记录\noptimize table 会强制进行merge之类的操作，使其达到最终状态\n查询某段时间范围内的CPU使用率 SQL如下:\nselect ts, avg(usage_user) from cpu where timestamp \u0026gt; \u0026#39;2022-01-15 06:59:59.","keywords":["ClickHouse"],"articleBody":"问个问题，如何优化一条SQL语句？我们首先想到的肯定是建索引。对于ClickHouse也不例外，尤其是稀疏主键索引（类似传统数据库中的主键索引）对性能的影响非常大。在下文中，我将结合例子对稀疏主键索引进行详细解读。\n注：本文内容主要参考官方文档，如果有余力，强烈建议先行阅读\n数据准备 这里，我将就我比较熟悉的时序数据进行举例。首先通过如下SQL建表：\n-- create dabase create database test; use test; -- create table CREATE TABLE cpu ( `hostname` String, `reginon` String, `datacenter` String, `timestamp` DateTime64(9,'Asia/Shanghai') CODEC (DoubleDelta), `usage_user` Float64, `usage_system` Float64 ) ENGINE = MergeTree() PRIMARY KEY tuple(); optimize table cpu final ; 并将本地的样本数据导入：\ncat example/output.csv |clickhouse-client -d test -q 'INSERT into cpu FORMAT CSV' output.csv是时序数据样本，时间间隔为1秒，包含了从2022-01-01 08:00:00到2022-01-15 07:59:59一共1209600条记录\noptimize table 会强制进行merge之类的操作，使其达到最终状态\n查询某段时间范围内的CPU使用率 SQL如下:\nselect ts, avg(usage_user) from cpu where timestamp \u003e '2022-01-15 06:59:59.000000000' and timestamp \u003c '2022-01-15 07:59:59.000000000' group by toStartOfMinute(timestamp) as ts order by ts; 统计结果如下：\n60 rows in set. Elapsed: 0.025 sec. Processed 1.21 million rows, 9.72 MB (48.68 million rows/s., 391.19 MB/s.) 注意：clickhouse客户端对每次查询会给出简要的性能数据，便于用户进行简单分析\n可以看到，尽管我们只查询了一个小时范围内的数据，但是依然扫描121万行数据，也就是进行了一次全表扫描！\n显然，这样的是不够的，我们需要建立合理的稀疏索引，而它将显著提高查询性能。\n稀疏主键索引 稀疏主键索引(Sparse Primary Indexes)，以下简称稀疏索引。其功能上类似于MySQL中的主键索引，不过实现原理上是截然不同的。\n长话短说，如若建立类似于B+树那种面向具体行的索引，在面对大数据场景时，必将占用大量内存和磁盘并且严重影响写入性能。此外，基于ClickHouse的实际使用场景考虑，也无需精确定位到每一行。\n因此，其总体设计上，ClickHouse将数据块按组（粒度）划分，并通过下标（mark）标记。这种设计使得索引的内存占用足够小，同时仍能显著提高查询性能，尤其是OLAP场景下的大数据量的范围查询和数据分析。\n配置Primary Key 稀疏索引可通过PRIMARY KEY语法指定，接下来让我们通过它来优化示例中的cpu表吧:\n-- create table with timestamp as primary key CREATE TABLE cpu_ts ( `hostname` String, `reginon` String, `datacenter` String, `timestamp` DateTime64(9,'Asia/Shanghai') CODEC (DoubleDelta), `usage_user` Float64, `usage_system` Float64 ) ENGINE = MergeTree() PRIMARY KEY (timestamp) ORDER BY (timestamp); -- insert data from cpu insert into cpu_ts select * from cpu; optimize table cpu_ts final ; 执行相同的分析SQL：\nselect ts, avg(usage_user) from cpu_ts where timestamp \u003e '2022-01-15 06:59:59.000000000' and timestamp \u003c '2022-01-15 07:59:59.000000000' group by toStartOfMinute(timestamp) as ts order by ts; 输出如下：\n60 rows in set. Elapsed: 0.004 sec. Processed 12.29 thousand rows, 196.58 KB (3.21 million rows/s., 51.31 MB/s.) 可以看到同样的结果，仅扫描5千多行! 下面，让我们通过EXPALIN来分析：\nEXPLAIN indexes = 1 select ts, avg(usage_user) from cpu_ts where timestamp \u003e '2022-01-15 06:59:59.000000000' and timestamp \u003c '2022-01-15 07:59:59.000000000' group by toStartOfMinute(timestamp) as ts order by ts; 输出如下：\n┌─explain────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ Expression (Projection) │ │ MergingSorted (Merge sorted streams for ORDER BY) │ │ MergeSorting (Merge sorted blocks for ORDER BY) │ │ PartialSorting (Sort each block for ORDER BY) │ │ Expression (Before ORDER BY) │ │ Aggregating │ │ Expression (Before GROUP BY) │ │ Filter (WHERE) │ │ SettingQuotaAndLimits (Set limits and quota after reading from storage) │ │ ReadFromMergeTree │ │ Indexes: │ │ PrimaryKey │ │ Keys: │ │ timestamp │ │ Condition: and((timestamp in (-inf, '1642204799.000000000')), (timestamp in ('1642201199.000000000', +inf))) │ │ Parts: 1/1 │ │ Granules: 1/148 │ └────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘ 17 rows in set. Elapsed: 0.003 sec. 可以看到，借助主键索引，我们仅仅扫描了一个粒度的数据，优化效果十分明显 ！\n实现原理 数据排布 我们不妨从实际磁盘上的数据文件来对ClickHouse有个总体上的理解。首先，让我们来看下cpu_ts表的某个partall_1_1_0/在磁盘上的存储结构：\nall_1_1_0其结构表示为： {分区}_{最小block数}_{最大block数}_{表示经历了几次merge}\n-rw-r----- 1 clickhouse clickhouse 589 Jun 26 05:42 checksums.txt -rw-r----- 1 clickhouse clickhouse 179 Jun 26 05:42 columns.txt -rw-r----- 1 clickhouse clickhouse 7 Jun 26 05:42 count.txt -rw-r----- 1 clickhouse clickhouse 55K Jun 26 05:42 datacenter.bin -rw-r----- 1 clickhouse clickhouse 3.4K Jun 26 05:42 datacenter.mrk2 -rw-r----- 1 clickhouse clickhouse 10 Jun 26 05:42 default_compression_codec.txt -rw-r----- 1 clickhouse clickhouse 34K Jun 26 05:42 hostname.bin -rw-r----- 1 clickhouse clickhouse 3.4K Jun 26 05:42 hostname.mrk2 -rw-r----- 1 clickhouse clickhouse 1.2K Jun 26 05:42 primary.idx -rw-r----- 1 clickhouse clickhouse 51K Jun 26 05:42 reginon.bin -rw-r----- 1 clickhouse clickhouse 3.4K Jun 26 05:42 reginon.mrk2 -rw-r----- 1 clickhouse clickhouse 147K Jun 26 05:42 timestamp.bin -rw-r----- 1 clickhouse clickhouse 3.4K Jun 26 05:42 timestamp.mrk2 -rw-r----- 1 clickhouse clickhouse 2.5M Jun 26 05:42 usage_system.bin -rw-r----- 1 clickhouse clickhouse 3.4K Jun 26 05:42 usage_system.mrk2 -rw-r----- 1 clickhouse clickhouse 2.5M Jun 26 05:42 usage_user.bin -rw-r----- 1 clickhouse clickhouse 3.4K Jun 26 05:42 usage_user.mrk2 其中主要文件如下:\n*.bin：每个表中的列上的值压缩后的数据文件 column.txt：此表中所有列以及每列的类型 default_compression_codec.txt：数据文件默认的压缩算法 primary.idx：数据索引，后文详解 *.mrk2：数据标记，后文详解 count.txt：此part中数据的行数 注意：通过指定的PRIMARY KEY，其数据行会以timestamp升序排列并生成主键索引，对于timestamp相同的行按照ORDER BY中的hostname进行(若PRIMARY KEY和ORDER BY分别指定，则PRIMARY KEY必须是ORDER BY的前缀)。\n索引建立 上文提到ClickHouse会对数据进行分组，其中某一列内的值自然也是按照组(粒度，默认8192行)进行划分的。粒度，可以简单理解为ClickHouse中一组不可划分的最小数据单元。\n当处理数据时，ClickHouse会读取整个粒度内的所有数据，而不是一行一行地读取（批处理的思想无处不在哈）。因此对于cpu_ts表内的数据，会被划分为148个组(ceil(1210000/8192=148)，其整体数据排布如下所示：\n从上图可以看出，表内数据被按照每8192行划分为148个粒度，每个组内取PRIMARY KEY中指定列的最小数据作为数据，并通过标记给其编号，存储在primary.idx中（可以将其理解为一个数组，标记就是下标）。我们使用od命令od -l -j 0 -N 32 primary.idx将primary.idx打印出来：\n0000000 1640995200000000000 1641003392000000000 0000020 1641011584000000000 1641019776000000000 # 粒度数组：[1640995200000000000, 1641003392000000000, 1641011584000000000, 1641019776000000000] (1642151554000000000-1642143362000000000)/1000000000 = 8192\n当我们根据timestamp字段进行过滤时，ClickHouse会通过二分搜索算法对primary.txt中的timestamp列进行搜索，找到对应的组标记，从而实现快速定位。例如对于示例查询语句，ClickHouse定位到的标记是148，那么又要如何通过标记反向找到实际的数据块呢？\n数据定位 这里，ClickHouse是通过引入{column_name}.mrk文件来解决的，mrk文件存储了粒度编号与压缩前后数据的偏移量(offset)，其大致结构如下图所示：\n使用od命令od -l -j 0 -N 24 timestamp.mrk2输出如下：\n0000000 0 0 0000020 8192 1071 0000040 0 8192 # (0, 0, 8192)为一组，表示压缩后偏移量为0，压缩前偏移量为0，粒度内一共8192行 # (1071, 0, 8192)为一组，表示压缩后偏移量为1071， 压缩前偏移量为0，粒度内一共8192行 在前文中，我们已经通过primary.idx已经拿到了具体的粒度编号(mark)，接着我们通过编号在{column}.mrk中找到对应的数据压缩前后的偏移量。然后通过以下2个步骤将数据发送给分析引擎：\n通过压缩后的偏移量定位到数据文件(*.bin)中的数据块并解压后加载到内存中 根据压缩前的偏移量定位到内存中相关未压缩的数据块，然后将其发送到分析引擎 总结 综上所述，ClickHouse的稀疏索引是综合权衡之下的产物，尽管其使用了一种看起来比较粗粒度的索引机制，但依然能获得达到相当客观的性能提升。毕竟默认8192行的粒度，对于动辄上亿级别的OLAP场景来说已经算是比较细粒度的了，同时得益于ClickHouse强大的并行计算与分析能力，其查询的性能需求是能够满足的。\n不过实际的使用场景中，由于PRIMARY KEY一旦定义就没法更改了，而实际的查询方式又往往是变化无常的。因此单靠稀疏索引有时无法满足实际需求。\nClickHouse为此额外提供了两种方案：一种是通过定义新的PRIMARY KEY，并通过创建新表或物化表之类来重建；而另外一种则是类似传统二级索引的机制叫做跳数索引来处理，这些我将在后序文章中进行介绍:)\n参考 https://github.com/timescale/tsbs https://clickhouse.com/docs/en/guides/improving-query-performance https://zhuanlan.zhihu.com/p/397411559 https://stackoverflow.com/questions/65198241/whats-the-process-of-clickhouse-primary-index ","wordCount":"591","inLanguage":"zh","datePublished":"2022-08-13T16:23:14+08:00","dateModified":"2022-08-13T16:23:14+08:00","author":{"@type":"Person","name":"wmingj"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://wmingj.github.io/posts/clickhouse-sparse-index/"},"publisher":{"@type":"Organization","name":"Wenming's Blog","logo":{"@type":"ImageObject","url":"https://wmingj.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://wmingj.github.io/ accesskey=h title="Wenming's Blog (Alt + H)">Wenming's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://wmingj.github.io/en/ title=English aria-label=English>En</a></li></ul></div></div><ul id=menu><li><a href=https://wmingj.github.io/search/ title="搜索 (Alt + /)" accesskey=/><span>搜索</span></a></li><li><a href=https://wmingj.github.io/archives/ title=归档><span>归档</span></a></li><li><a href=https://wmingj.github.io/tags/ title=标签><span>标签</span></a></li><li><a href=https://wmingj.github.io/readings/ title=书单><span>书单</span></a></li><li><a href=https://wmingj.github.io/about/ title=关于><span>关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">ClickHouse稀疏索引原理解读</h1><div class=post-meta><span title='2022-08-13 16:23:14 +0800 +0800'>八月 13, 2022</span>&nbsp;·&nbsp;3 分钟&nbsp;·&nbsp;wmingj</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>目录</span></summary><div class=inner><ul><li><a href=#%e6%95%b0%e6%8d%ae%e5%87%86%e5%a4%87 aria-label=数据准备>数据准备</a><ul><li><a href=#%e6%9f%a5%e8%af%a2%e6%9f%90%e6%ae%b5%e6%97%b6%e9%97%b4%e8%8c%83%e5%9b%b4%e5%86%85%e7%9a%84cpu%e4%bd%bf%e7%94%a8%e7%8e%87 aria-label=查询某段时间范围内的CPU使用率>查询某段时间范围内的CPU使用率</a></li></ul></li><li><a href=#%e7%a8%80%e7%96%8f%e4%b8%bb%e9%94%ae%e7%b4%a2%e5%bc%95 aria-label=稀疏主键索引>稀疏主键索引</a><ul><li><a href=#%e9%85%8d%e7%bd%aeprimary-key aria-label="配置Primary Key">配置Primary Key</a></li><li><a href=#%e5%ae%9e%e7%8e%b0%e5%8e%9f%e7%90%86 aria-label=实现原理>实现原理</a><ul><li><a href=#%e6%95%b0%e6%8d%ae%e6%8e%92%e5%b8%83 aria-label=数据排布>数据排布</a></li><li><a href=#%e7%b4%a2%e5%bc%95%e5%bb%ba%e7%ab%8b aria-label=索引建立>索引建立</a></li><li><a href=#%e6%95%b0%e6%8d%ae%e5%ae%9a%e4%bd%8d aria-label=数据定位>数据定位</a></li></ul></li></ul></li><li><a href=#%e6%80%bb%e7%bb%93 aria-label=总结>总结</a></li><li><a href=#%e5%8f%82%e8%80%83 aria-label=参考>参考</a></li></ul></div></details></div><div class=post-content><p>问个问题，如何优化一条SQL语句？我们首先想到的肯定是建索引。对于ClickHouse也不例外，尤其是稀疏主键索引（类似传统数据库中的主键索引）对性能的影响非常大。在下文中，我将结合例子对稀疏主键索引进行详细解读。</p><blockquote><p>注：本文内容主要参考<a href=https://clickhouse.com/docs/en/guides/improving-query-performance>官方文档</a>，如果有余力，强烈建议先行阅读</p></blockquote><h1 id=数据准备>数据准备<a hidden class=anchor aria-hidden=true href=#数据准备>#</a></h1><p>这里，我将就我比较熟悉的时序数据进行举例。首先通过如下SQL建表：</p><pre tabindex=0><code>-- create dabase
create database test;
use test;
-- create table
CREATE TABLE cpu
(
    `hostname`     String,
    `reginon`      String,
    `datacenter`   String,
    `timestamp`    DateTime64(9,&#39;Asia/Shanghai&#39;) CODEC (DoubleDelta),
    `usage_user`   Float64,
    `usage_system` Float64
)
ENGINE = MergeTree() PRIMARY KEY tuple();

optimize table cpu final ;
</code></pre><p>并将本地的样本数据导入：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>cat example/output.csv |clickhouse-client -d test -q <span style=color:#e6db74>&#39;INSERT into cpu FORMAT CSV&#39;</span>
</span></span></code></pre></div><blockquote><ul><li><p><a href=https://github.com/erenming/data-pool/blob/main/data/output.tar.gz>output.csv</a>是时序数据样本，时间间隔为1秒，包含了从2022-01-01 08:00:00到2022-01-15 07:59:59一共1209600条记录</p></li><li><p><a href=https://clickhouse.com/docs/en/sql-reference/statements/optimize/>optimize table</a> 会强制进行merge之类的操作，使其达到最终状态</p></li></ul></blockquote><h2 id=查询某段时间范围内的cpu使用率>查询某段时间范围内的CPU使用率<a hidden class=anchor aria-hidden=true href=#查询某段时间范围内的cpu使用率>#</a></h2><p>SQL如下:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>select</span> ts, <span style=color:#66d9ef>avg</span>(usage_user) <span style=color:#66d9ef>from</span> cpu <span style=color:#66d9ef>where</span> <span style=color:#66d9ef>timestamp</span> <span style=color:#f92672>&gt;</span> <span style=color:#e6db74>&#39;2022-01-15 06:59:59.000000000&#39;</span> <span style=color:#66d9ef>and</span> <span style=color:#66d9ef>timestamp</span> <span style=color:#f92672>&lt;</span> <span style=color:#e6db74>&#39;2022-01-15 07:59:59.000000000&#39;</span> <span style=color:#66d9ef>group</span> <span style=color:#66d9ef>by</span> toStartOfMinute(<span style=color:#66d9ef>timestamp</span>) <span style=color:#66d9ef>as</span> ts <span style=color:#66d9ef>order</span> <span style=color:#66d9ef>by</span> ts;
</span></span></code></pre></div><p>统计结果如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#ae81ff>60</span> rows in set. Elapsed: 0.025 sec. Processed 1.21 million rows, 9.72 MB <span style=color:#f92672>(</span>48.68 million rows/s., 391.19 MB/s.<span style=color:#f92672>)</span>
</span></span></code></pre></div><blockquote><p>注意：clickhouse客户端对每次查询会给出简要的性能数据，便于用户进行简单分析</p></blockquote><p>可以看到，尽管我们只查询了一个小时范围内的数据，但是依然扫描121万行数据，也就是进行了一次<strong>全表扫描！</strong></p><p>显然，这样的是不够的，我们需要建立合理的稀疏索引，而它将显著提高查询性能。</p><h1 id=稀疏主键索引>稀疏主键索引<a hidden class=anchor aria-hidden=true href=#稀疏主键索引>#</a></h1><p>稀疏主键索引(Sparse Primary Indexes)，以下简称稀疏索引。其功能上类似于MySQL中的主键索引，不过实现原理上是截然不同的。</p><p>长话短说，如若建立类似于B+树那种面向具体行的索引，在面对大数据场景时，必将占用大量内存和磁盘并且严重影响写入性能。此外，基于ClickHouse的实际使用场景考虑，也无需精确定位到每一行。</p><p>因此，其总体设计上，ClickHouse将数据块按组（<strong>粒度</strong>）划分，并通过下标（<strong>mark</strong>）标记。这种设计使得索引的内存占用足够小，同时仍能显著提高查询性能，尤其是OLAP场景下的大数据量的范围查询和数据分析。</p><h2 id=配置primary-key>配置Primary Key<a hidden class=anchor aria-hidden=true href=#配置primary-key>#</a></h2><p>稀疏索引可通过<a href=https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree/#mergetree-query-clauses>PRIMARY KEY</a>语法指定，接下来让我们通过它来优化示例中的cpu表吧:</p><pre tabindex=0><code>-- create table with timestamp as primary key
CREATE TABLE cpu_ts
(
    `hostname`     String,
    `reginon`      String,
    `datacenter`   String,
    `timestamp`    DateTime64(9,&#39;Asia/Shanghai&#39;) CODEC (DoubleDelta),
    `usage_user`   Float64,
    `usage_system` Float64
)
ENGINE = MergeTree()
PRIMARY KEY (timestamp)
ORDER BY (timestamp);

-- insert data from cpu
insert into cpu_ts select * from cpu;

optimize table cpu_ts final ;
</code></pre><p>执行相同的分析SQL：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>select</span> ts, <span style=color:#66d9ef>avg</span>(usage_user) <span style=color:#66d9ef>from</span> cpu_ts <span style=color:#66d9ef>where</span> <span style=color:#66d9ef>timestamp</span> <span style=color:#f92672>&gt;</span> <span style=color:#e6db74>&#39;2022-01-15 06:59:59.000000000&#39;</span> <span style=color:#66d9ef>and</span> <span style=color:#66d9ef>timestamp</span> <span style=color:#f92672>&lt;</span> <span style=color:#e6db74>&#39;2022-01-15 07:59:59.000000000&#39;</span> <span style=color:#66d9ef>group</span> <span style=color:#66d9ef>by</span> toStartOfMinute(<span style=color:#66d9ef>timestamp</span>) <span style=color:#66d9ef>as</span> ts <span style=color:#66d9ef>order</span> <span style=color:#66d9ef>by</span> ts;
</span></span></code></pre></div><p>输出如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#ae81ff>60</span> rows in set. Elapsed: 0.004 sec. Processed 12.29 thousand rows, 196.58 KB <span style=color:#f92672>(</span>3.21 million rows/s., 51.31 MB/s.<span style=color:#f92672>)</span>
</span></span></code></pre></div><p>可以看到同样的结果，<strong>仅扫描5千多行!</strong> 下面，让我们通过EXPALIN来分析：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>EXPLAIN</span> indexes <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span> <span style=color:#66d9ef>select</span> ts, <span style=color:#66d9ef>avg</span>(usage_user) <span style=color:#66d9ef>from</span> cpu_ts <span style=color:#66d9ef>where</span> <span style=color:#66d9ef>timestamp</span> <span style=color:#f92672>&gt;</span> <span style=color:#e6db74>&#39;2022-01-15 06:59:59.000000000&#39;</span> <span style=color:#66d9ef>and</span> <span style=color:#66d9ef>timestamp</span> <span style=color:#f92672>&lt;</span> <span style=color:#e6db74>&#39;2022-01-15 07:59:59.000000000&#39;</span> <span style=color:#66d9ef>group</span> <span style=color:#66d9ef>by</span> toStartOfMinute(<span style=color:#66d9ef>timestamp</span>) <span style=color:#66d9ef>as</span> ts <span style=color:#66d9ef>order</span> <span style=color:#66d9ef>by</span> ts;
</span></span></code></pre></div><p>输出如下：</p><pre tabindex=0><code>┌─explain────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Expression (Projection)                                                                                                            │
│   MergingSorted (Merge sorted streams for ORDER BY)                                                                                │
│     MergeSorting (Merge sorted blocks for ORDER BY)                                                                                │
│       PartialSorting (Sort each block for ORDER BY)                                                                                │
│         Expression (Before ORDER BY)                                                                                               │
│           Aggregating                                                                                                              │
│             Expression (Before GROUP BY)                                                                                           │
│               Filter (WHERE)                                                                                                       │
│                 SettingQuotaAndLimits (Set limits and quota after reading from storage)                                            │
│                   ReadFromMergeTree                                                                                                │
│                   Indexes:                                                                                                         │
│                     PrimaryKey                                                                                                     │
│                       Keys:                                                                                                        │
│                         timestamp                                                                                                  │
│                       Condition: and((timestamp in (-inf, &#39;1642204799.000000000&#39;)), (timestamp in (&#39;1642201199.000000000&#39;, +inf))) │
│                       Parts: 1/1                                                                                                   │
│                       Granules: 1/148                                                                                              │
└────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘

17 rows in set. Elapsed: 0.003 sec. 
</code></pre><p>可以看到，借助主键索引，我们仅仅扫描了一个粒度的数据，<strong>优化效果十分明显 ！</strong></p><h2 id=实现原理>实现原理<a hidden class=anchor aria-hidden=true href=#实现原理>#</a></h2><h3 id=数据排布>数据排布<a hidden class=anchor aria-hidden=true href=#数据排布>#</a></h3><p>我们不妨从实际磁盘上的数据文件来对ClickHouse有个总体上的理解。首先，让我们来看下cpu_ts表的某个part<code>all_1_1_0/</code>在磁盘上的存储结构：</p><blockquote><p>all_1_1_0其结构表示为： {分区}_{最小block数}_{最大block数}_{表示经历了几次merge}</p></blockquote><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>-rw-r----- <span style=color:#ae81ff>1</span> clickhouse clickhouse  <span style=color:#ae81ff>589</span> Jun <span style=color:#ae81ff>26</span> 05:42 checksums.txt
</span></span><span style=display:flex><span>-rw-r----- <span style=color:#ae81ff>1</span> clickhouse clickhouse  <span style=color:#ae81ff>179</span> Jun <span style=color:#ae81ff>26</span> 05:42 columns.txt
</span></span><span style=display:flex><span>-rw-r----- <span style=color:#ae81ff>1</span> clickhouse clickhouse    <span style=color:#ae81ff>7</span> Jun <span style=color:#ae81ff>26</span> 05:42 count.txt
</span></span><span style=display:flex><span>-rw-r----- <span style=color:#ae81ff>1</span> clickhouse clickhouse  55K Jun <span style=color:#ae81ff>26</span> 05:42 datacenter.bin
</span></span><span style=display:flex><span>-rw-r----- <span style=color:#ae81ff>1</span> clickhouse clickhouse 3.4K Jun <span style=color:#ae81ff>26</span> 05:42 datacenter.mrk2
</span></span><span style=display:flex><span>-rw-r----- <span style=color:#ae81ff>1</span> clickhouse clickhouse   <span style=color:#ae81ff>10</span> Jun <span style=color:#ae81ff>26</span> 05:42 default_compression_codec.txt
</span></span><span style=display:flex><span>-rw-r----- <span style=color:#ae81ff>1</span> clickhouse clickhouse  34K Jun <span style=color:#ae81ff>26</span> 05:42 hostname.bin
</span></span><span style=display:flex><span>-rw-r----- <span style=color:#ae81ff>1</span> clickhouse clickhouse 3.4K Jun <span style=color:#ae81ff>26</span> 05:42 hostname.mrk2
</span></span><span style=display:flex><span>-rw-r----- <span style=color:#ae81ff>1</span> clickhouse clickhouse 1.2K Jun <span style=color:#ae81ff>26</span> 05:42 primary.idx
</span></span><span style=display:flex><span>-rw-r----- <span style=color:#ae81ff>1</span> clickhouse clickhouse  51K Jun <span style=color:#ae81ff>26</span> 05:42 reginon.bin
</span></span><span style=display:flex><span>-rw-r----- <span style=color:#ae81ff>1</span> clickhouse clickhouse 3.4K Jun <span style=color:#ae81ff>26</span> 05:42 reginon.mrk2
</span></span><span style=display:flex><span>-rw-r----- <span style=color:#ae81ff>1</span> clickhouse clickhouse 147K Jun <span style=color:#ae81ff>26</span> 05:42 timestamp.bin
</span></span><span style=display:flex><span>-rw-r----- <span style=color:#ae81ff>1</span> clickhouse clickhouse 3.4K Jun <span style=color:#ae81ff>26</span> 05:42 timestamp.mrk2
</span></span><span style=display:flex><span>-rw-r----- <span style=color:#ae81ff>1</span> clickhouse clickhouse 2.5M Jun <span style=color:#ae81ff>26</span> 05:42 usage_system.bin
</span></span><span style=display:flex><span>-rw-r----- <span style=color:#ae81ff>1</span> clickhouse clickhouse 3.4K Jun <span style=color:#ae81ff>26</span> 05:42 usage_system.mrk2
</span></span><span style=display:flex><span>-rw-r----- <span style=color:#ae81ff>1</span> clickhouse clickhouse 2.5M Jun <span style=color:#ae81ff>26</span> 05:42 usage_user.bin
</span></span><span style=display:flex><span>-rw-r----- <span style=color:#ae81ff>1</span> clickhouse clickhouse 3.4K Jun <span style=color:#ae81ff>26</span> 05:42 usage_user.mrk2
</span></span></code></pre></div><p>其中主要文件如下:</p><ul><li>*.bin：每个表中的列上的值压缩后的数据文件</li><li>column.txt：此表中所有列以及每列的类型</li><li>default_compression_codec.txt：数据文件默认的压缩算法</li><li>primary.idx：数据索引，后文详解</li><li>*.mrk2：数据标记，后文详解</li><li>count.txt：此part中数据的行数</li></ul><blockquote><p><strong>注意</strong>：通过指定的PRIMARY KEY，其数据行会以timestamp升序排列并生成主键索引，对于timestamp相同的行按照ORDER BY中的hostname进行(若PRIMARY KEY和ORDER BY分别指定，则PRIMARY KEY必须是ORDER BY的前缀)。</p></blockquote><h3 id=索引建立>索引建立<a hidden class=anchor aria-hidden=true href=#索引建立>#</a></h3><p>上文提到ClickHouse会对数据进行分组，其中某一列内的值自然也是按照组(粒度，默认8192行)进行划分的。<strong>粒度</strong>，可以简单理解为ClickHouse中一组不可划分的最小数据单元。</p><p>当处理数据时，ClickHouse会读取整个粒度内的所有数据，而不是一行一行地读取（批处理的思想无处不在哈）。因此对于cpu_ts表内的数据，会被划分为148个组(ceil(1210000/8192=148)，其整体<strong>数据排布</strong>如下所示：</p><p><img loading=lazy src=https://raw.githubusercontent.com/erenming/image-pool/master/blog/image-20220628222744904.png alt=image-20220628222744904></p><p>从上图可以看出，表内数据被按照每8192行划分为148个粒度，每个组内取PRIMARY KEY中指定列的最小数据作为数据，并通过标记给其编号，存储在<code>primary.idx</code>中（可以将其理解为一个数组，标记就是下标）。我们使用od命令<code>od -l -j 0 -N 32 primary.idx</code>将primary.idx打印出来：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#ae81ff>0000000</span>  <span style=color:#ae81ff>1640995200000000000</span>  <span style=color:#ae81ff>1641003392000000000</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>0000020</span>  <span style=color:#ae81ff>1641011584000000000</span>  <span style=color:#ae81ff>1641019776000000000</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 粒度数组：[1640995200000000000, 1641003392000000000, 1641011584000000000, 1641019776000000000]</span>
</span></span></code></pre></div><blockquote><p>(1642151554000000000-1642143362000000000)/1000000000 = 8192</p></blockquote><p>当我们根据timestamp字段进行过滤时，ClickHouse会通过<strong>二分搜索算法</strong>对primary.txt中的timestamp列进行搜索，找到对应的组标记，从而实现快速定位。例如对于示例查询语句，ClickHouse定位到的标记是148，那么又要如何通过标记反向找到实际的数据块呢？</p><h3 id=数据定位>数据定位<a hidden class=anchor aria-hidden=true href=#数据定位>#</a></h3><p>这里，ClickHouse是通过引入<code>{column_name}.mrk</code>文件来解决的，mrk文件存储了粒度编号与压缩前后数据的偏移量(offset)，其大致结构如下图所示：</p><p><img loading=lazy src=https://raw.githubusercontent.com/erenming/image-pool/master/blog/image-20220626231420765.png alt=image-20220626231420765></p><p>使用od命令<code>od -l -j 0 -N 24 timestamp.mrk2</code>输出如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#ae81ff>0000000</span>                    <span style=color:#ae81ff>0</span>                    <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>0000020</span>                 <span style=color:#ae81ff>8192</span>                 <span style=color:#ae81ff>1071</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>0000040</span>                    <span style=color:#ae81ff>0</span>                 <span style=color:#ae81ff>8192</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># (0, 0, 8192)为一组，表示压缩后偏移量为0，压缩前偏移量为0，粒度内一共8192行</span>
</span></span><span style=display:flex><span><span style=color:#75715e># (1071, 0, 8192)为一组，表示压缩后偏移量为1071， 压缩前偏移量为0，粒度内一共8192行</span>
</span></span></code></pre></div><p>在前文中，我们已经通过primary.idx已经拿到了具体的粒度编号(mark)，接着我们通过编号在{column}.mrk中找到对应的数据压缩前后的偏移量。然后通过以下2个步骤将数据发送给分析引擎：</p><ol><li>通过<strong>压缩后的偏移量</strong>定位到数据文件(*.bin)中的数据块并解压后加载到内存中</li><li>根据<strong>压缩前的偏移量</strong>定位到内存中相关未压缩的数据块，然后将其发送到分析引擎</li></ol><p><img loading=lazy src=https://raw.githubusercontent.com/erenming/image-pool/master/blog/image-20220628223322552.png alt=image-20220628223322552></p><h1 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h1><p>综上所述，ClickHouse的稀疏索引是综合权衡之下的产物，尽管其使用了一种看起来比较粗粒度的索引机制，但依然能获得达到相当客观的性能提升。毕竟默认8192行的粒度，对于动辄上亿级别的OLAP场景来说已经算是比较细粒度的了，同时得益于ClickHouse强大的并行计算与分析能力，其查询的性能需求是能够满足的。</p><p>不过实际的使用场景中，由于PRIMARY KEY一旦定义就没法更改了，而实际的查询方式又往往是变化无常的。因此单靠稀疏索引有时无法满足实际需求。</p><p>ClickHouse为此额外提供了两种方案：一种是通过定义新的PRIMARY KEY，并通过创建新表或物化表之类来重建；而另外一种则是类似传统二级索引的机制叫做<strong>跳数索引</strong>来处理，这些我将在后序文章中进行介绍:)</p><h1 id=参考>参考<a hidden class=anchor aria-hidden=true href=#参考>#</a></h1><ul><li><a href=https://github.com/timescale/tsbs>https://github.com/timescale/tsbs</a></li><li><a href=https://clickhouse.com/docs/en/guides/improving-query-performance>https://clickhouse.com/docs/en/guides/improving-query-performance</a></li><li><a href=https://zhuanlan.zhihu.com/p/397411559>https://zhuanlan.zhihu.com/p/397411559</a></li><li><a href=https://stackoverflow.com/questions/65198241/whats-the-process-of-clickhouse-primary-index>https://stackoverflow.com/questions/65198241/whats-the-process-of-clickhouse-primary-index</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://wmingj.github.io/tags/clickhouse/>ClickHouse</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://wmingj.github.io/>Wenming's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>