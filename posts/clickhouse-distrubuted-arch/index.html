<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>ClickHouse的分布式实现 | Nothing Special</title>
<meta name=keywords content="Clickhouse"><meta name=description content="当我们需要在实际生产环境中使用ClickHouse时，高可用与可扩展是绕不开的话题，因此ClickHouse也提供了分布式的相关机制来应对这些问题。在下文中，我们将主要从副本机制、分片机制两个个方面来对齐进行介绍。
副本机制 ClickHouse通过扩展MergeTree为ReplicatedMergeTree来创建副本表引擎（通过在MergeTree添加Replicated前缀来表示副本表引擎）。这里需要注意的是，副本表并非一种具体的表引擎，而是一种逻辑上的表引擎，实际数据的存取仍然通过MergeTree来完成。
注意：这里，我们假定集群名为local，且包含两个节点chi-0和chi-1
建表 ReplicatedMergeTree通过类似如下语句进行创建：
CREATE TABLE table_name ( EventDate DateTime, CounterID UInt32, UserID UInt32, ver UInt16 ) ENGINE = ReplicatedReplacingMergeTree('/clickhouse/tables/{cluster}-{shard}/table_name', '{replica}', ver) PARTITION BY toYYYYMM(EventDate) ORDER BY (CounterID, EventDate, intHash32(UserID)) SAMPLE BY intHash32(UserID); 有两个参数需要重点说明一下，分别为zoo_path和replica_name参数：
zoo_path: 表示表所在的zk路径 replica_name: 表示副本名称，通常为主机名 ClickHouse会在zk中建立路径zoo_path，并在zoo_path的子目录/replicas下根据replica_name创建副本标识，因此可以看到replica_name参数的作用主要就是用来作为副本ID。
我们这里假定首先在chi-0节点上执行了建表语句
其首先创建一个副本实例，进行一些初始化的工作，在zk上创建相关节点 接着在/replicas节点下注册副本实例chi-0 启用监听任务，监听/log节点 参与leader节点选举(通过向/leader_election写入数据，谁先写入成功谁就是leader) 接着，我们在chi-1节点上执行建表语句：
首先也是创建副本实例，进行初始化工作 接着在/replicas节点下注册副本实例chi-1 启用监听任务，监听/log节点 参与leader节点选举(此时由于chi-0节点上已经执行过建表流程了，因此chi-0为leader副本) /log节点非常重要，用来记录各种操作LogEntry包括获取part，合并part，删除分区等等操作
写入 接着，我们通过执行INSERT INTO语句向chi-0节点写入数据（当写入请求被发到从节点时，从节点会将其转发到主节点）。
此时，会首先在本地完成分区数据的写入，然后向/blocks节点写入该分区的block_id
block是ClickHouse中的最小数据单元，这里在/blocks节点中写入block_id主要是为了后续数据的去重
接着向/log节点推送日志，日志信息如下所示：
format version: 4 create_time: 2022-09-04 14:30:58 source replica: chi-0 block_id: 20220904_5211346952104599192_1472622755444261990 get 20220904_269677_269677_0 part_type: Compact ."><meta name=author content="erenming"><link rel=canonical href=https://erenming.github.io/posts/clickhouse-distrubuted-arch/><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=https://erenming.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://erenming.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://erenming.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://erenming.github.io/apple-touch-icon.png><link rel=mask-icon href=https://erenming.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://erenming.github.io/posts/clickhouse-distrubuted-arch/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="ClickHouse的分布式实现"><meta property="og:description" content="当我们需要在实际生产环境中使用ClickHouse时，高可用与可扩展是绕不开的话题，因此ClickHouse也提供了分布式的相关机制来应对这些问题。在下文中，我们将主要从副本机制、分片机制两个个方面来对齐进行介绍。
副本机制 ClickHouse通过扩展MergeTree为ReplicatedMergeTree来创建副本表引擎（通过在MergeTree添加Replicated前缀来表示副本表引擎）。这里需要注意的是，副本表并非一种具体的表引擎，而是一种逻辑上的表引擎，实际数据的存取仍然通过MergeTree来完成。
注意：这里，我们假定集群名为local，且包含两个节点chi-0和chi-1
建表 ReplicatedMergeTree通过类似如下语句进行创建：
CREATE TABLE table_name ( EventDate DateTime, CounterID UInt32, UserID UInt32, ver UInt16 ) ENGINE = ReplicatedReplacingMergeTree('/clickhouse/tables/{cluster}-{shard}/table_name', '{replica}', ver) PARTITION BY toYYYYMM(EventDate) ORDER BY (CounterID, EventDate, intHash32(UserID)) SAMPLE BY intHash32(UserID); 有两个参数需要重点说明一下，分别为zoo_path和replica_name参数：
zoo_path: 表示表所在的zk路径 replica_name: 表示副本名称，通常为主机名 ClickHouse会在zk中建立路径zoo_path，并在zoo_path的子目录/replicas下根据replica_name创建副本标识，因此可以看到replica_name参数的作用主要就是用来作为副本ID。
我们这里假定首先在chi-0节点上执行了建表语句
其首先创建一个副本实例，进行一些初始化的工作，在zk上创建相关节点 接着在/replicas节点下注册副本实例chi-0 启用监听任务，监听/log节点 参与leader节点选举(通过向/leader_election写入数据，谁先写入成功谁就是leader) 接着，我们在chi-1节点上执行建表语句：
首先也是创建副本实例，进行初始化工作 接着在/replicas节点下注册副本实例chi-1 启用监听任务，监听/log节点 参与leader节点选举(此时由于chi-0节点上已经执行过建表流程了，因此chi-0为leader副本) /log节点非常重要，用来记录各种操作LogEntry包括获取part，合并part，删除分区等等操作
写入 接着，我们通过执行INSERT INTO语句向chi-0节点写入数据（当写入请求被发到从节点时，从节点会将其转发到主节点）。
此时，会首先在本地完成分区数据的写入，然后向/blocks节点写入该分区的block_id
block是ClickHouse中的最小数据单元，这里在/blocks节点中写入block_id主要是为了后续数据的去重
接着向/log节点推送日志，日志信息如下所示：
format version: 4 create_time: 2022-09-04 14:30:58 source replica: chi-0 block_id: 20220904_5211346952104599192_1472622755444261990 get 20220904_269677_269677_0 part_type: Compact ."><meta property="og:type" content="article"><meta property="og:url" content="https://erenming.github.io/posts/clickhouse-distrubuted-arch/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-04-10T16:24:04+08:00"><meta property="article:modified_time" content="2023-04-10T16:24:04+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="ClickHouse的分布式实现"><meta name=twitter:description content="当我们需要在实际生产环境中使用ClickHouse时，高可用与可扩展是绕不开的话题，因此ClickHouse也提供了分布式的相关机制来应对这些问题。在下文中，我们将主要从副本机制、分片机制两个个方面来对齐进行介绍。
副本机制 ClickHouse通过扩展MergeTree为ReplicatedMergeTree来创建副本表引擎（通过在MergeTree添加Replicated前缀来表示副本表引擎）。这里需要注意的是，副本表并非一种具体的表引擎，而是一种逻辑上的表引擎，实际数据的存取仍然通过MergeTree来完成。
注意：这里，我们假定集群名为local，且包含两个节点chi-0和chi-1
建表 ReplicatedMergeTree通过类似如下语句进行创建：
CREATE TABLE table_name ( EventDate DateTime, CounterID UInt32, UserID UInt32, ver UInt16 ) ENGINE = ReplicatedReplacingMergeTree('/clickhouse/tables/{cluster}-{shard}/table_name', '{replica}', ver) PARTITION BY toYYYYMM(EventDate) ORDER BY (CounterID, EventDate, intHash32(UserID)) SAMPLE BY intHash32(UserID); 有两个参数需要重点说明一下，分别为zoo_path和replica_name参数：
zoo_path: 表示表所在的zk路径 replica_name: 表示副本名称，通常为主机名 ClickHouse会在zk中建立路径zoo_path，并在zoo_path的子目录/replicas下根据replica_name创建副本标识，因此可以看到replica_name参数的作用主要就是用来作为副本ID。
我们这里假定首先在chi-0节点上执行了建表语句
其首先创建一个副本实例，进行一些初始化的工作，在zk上创建相关节点 接着在/replicas节点下注册副本实例chi-0 启用监听任务，监听/log节点 参与leader节点选举(通过向/leader_election写入数据，谁先写入成功谁就是leader) 接着，我们在chi-1节点上执行建表语句：
首先也是创建副本实例，进行初始化工作 接着在/replicas节点下注册副本实例chi-1 启用监听任务，监听/log节点 参与leader节点选举(此时由于chi-0节点上已经执行过建表流程了，因此chi-0为leader副本) /log节点非常重要，用来记录各种操作LogEntry包括获取part，合并part，删除分区等等操作
写入 接着，我们通过执行INSERT INTO语句向chi-0节点写入数据（当写入请求被发到从节点时，从节点会将其转发到主节点）。
此时，会首先在本地完成分区数据的写入，然后向/blocks节点写入该分区的block_id
block是ClickHouse中的最小数据单元，这里在/blocks节点中写入block_id主要是为了后续数据的去重
接着向/log节点推送日志，日志信息如下所示：
format version: 4 create_time: 2022-09-04 14:30:58 source replica: chi-0 block_id: 20220904_5211346952104599192_1472622755444261990 get 20220904_269677_269677_0 part_type: Compact ."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://erenming.github.io/posts/"},{"@type":"ListItem","position":2,"name":"ClickHouse的分布式实现","item":"https://erenming.github.io/posts/clickhouse-distrubuted-arch/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"ClickHouse的分布式实现","name":"ClickHouse的分布式实现","description":"当我们需要在实际生产环境中使用ClickHouse时，高可用与可扩展是绕不开的话题，因此ClickHouse也提供了分布式的相关机制来应对这些问题。在下文中，我们将主要从副本机制、分片机制两个个方面来对齐进行介绍。\n副本机制 ClickHouse通过扩展MergeTree为ReplicatedMergeTree来创建副本表引擎（通过在MergeTree添加Replicated前缀来表示副本表引擎）。这里需要注意的是，副本表并非一种具体的表引擎，而是一种逻辑上的表引擎，实际数据的存取仍然通过MergeTree来完成。\n注意：这里，我们假定集群名为local，且包含两个节点chi-0和chi-1\n建表 ReplicatedMergeTree通过类似如下语句进行创建：\nCREATE TABLE table_name ( EventDate DateTime, CounterID UInt32, UserID UInt32, ver UInt16 ) ENGINE = ReplicatedReplacingMergeTree(\u0026#39;/clickhouse/tables/{cluster}-{shard}/table_name\u0026#39;, \u0026#39;{replica}\u0026#39;, ver) PARTITION BY toYYYYMM(EventDate) ORDER BY (CounterID, EventDate, intHash32(UserID)) SAMPLE BY intHash32(UserID); 有两个参数需要重点说明一下，分别为zoo_path和replica_name参数：\nzoo_path: 表示表所在的zk路径 replica_name: 表示副本名称，通常为主机名 ClickHouse会在zk中建立路径zoo_path，并在zoo_path的子目录/replicas下根据replica_name创建副本标识，因此可以看到replica_name参数的作用主要就是用来作为副本ID。\n我们这里假定首先在chi-0节点上执行了建表语句\n其首先创建一个副本实例，进行一些初始化的工作，在zk上创建相关节点 接着在/replicas节点下注册副本实例chi-0 启用监听任务，监听/log节点 参与leader节点选举(通过向/leader_election写入数据，谁先写入成功谁就是leader) 接着，我们在chi-1节点上执行建表语句：\n首先也是创建副本实例，进行初始化工作 接着在/replicas节点下注册副本实例chi-1 启用监听任务，监听/log节点 参与leader节点选举(此时由于chi-0节点上已经执行过建表流程了，因此chi-0为leader副本) /log节点非常重要，用来记录各种操作LogEntry包括获取part，合并part，删除分区等等操作\n写入 接着，我们通过执行INSERT INTO语句向chi-0节点写入数据（当写入请求被发到从节点时，从节点会将其转发到主节点）。\n此时，会首先在本地完成分区数据的写入，然后向/blocks节点写入该分区的block_id\nblock是ClickHouse中的最小数据单元，这里在/blocks节点中写入block_id主要是为了后续数据的去重\n接着向/log节点推送日志，日志信息如下所示：\nformat version: 4 create_time: 2022-09-04 14:30:58 source replica: chi-0 block_id: 20220904_5211346952104599192_1472622755444261990 get 20220904_269677_269677_0 part_type: Compact .","keywords":["Clickhouse"],"articleBody":"当我们需要在实际生产环境中使用ClickHouse时，高可用与可扩展是绕不开的话题，因此ClickHouse也提供了分布式的相关机制来应对这些问题。在下文中，我们将主要从副本机制、分片机制两个个方面来对齐进行介绍。\n副本机制 ClickHouse通过扩展MergeTree为ReplicatedMergeTree来创建副本表引擎（通过在MergeTree添加Replicated前缀来表示副本表引擎）。这里需要注意的是，副本表并非一种具体的表引擎，而是一种逻辑上的表引擎，实际数据的存取仍然通过MergeTree来完成。\n注意：这里，我们假定集群名为local，且包含两个节点chi-0和chi-1\n建表 ReplicatedMergeTree通过类似如下语句进行创建：\nCREATE TABLE table_name ( EventDate DateTime, CounterID UInt32, UserID UInt32, ver UInt16 ) ENGINE = ReplicatedReplacingMergeTree('/clickhouse/tables/{cluster}-{shard}/table_name', '{replica}', ver) PARTITION BY toYYYYMM(EventDate) ORDER BY (CounterID, EventDate, intHash32(UserID)) SAMPLE BY intHash32(UserID); 有两个参数需要重点说明一下，分别为zoo_path和replica_name参数：\nzoo_path: 表示表所在的zk路径 replica_name: 表示副本名称，通常为主机名 ClickHouse会在zk中建立路径zoo_path，并在zoo_path的子目录/replicas下根据replica_name创建副本标识，因此可以看到replica_name参数的作用主要就是用来作为副本ID。\n我们这里假定首先在chi-0节点上执行了建表语句\n其首先创建一个副本实例，进行一些初始化的工作，在zk上创建相关节点 接着在/replicas节点下注册副本实例chi-0 启用监听任务，监听/log节点 参与leader节点选举(通过向/leader_election写入数据，谁先写入成功谁就是leader) 接着，我们在chi-1节点上执行建表语句：\n首先也是创建副本实例，进行初始化工作 接着在/replicas节点下注册副本实例chi-1 启用监听任务，监听/log节点 参与leader节点选举(此时由于chi-0节点上已经执行过建表流程了，因此chi-0为leader副本) /log节点非常重要，用来记录各种操作LogEntry包括获取part，合并part，删除分区等等操作\n写入 接着，我们通过执行INSERT INTO语句向chi-0节点写入数据（当写入请求被发到从节点时，从节点会将其转发到主节点）。\n此时，会首先在本地完成分区数据的写入，然后向/blocks节点写入该分区的block_id\nblock是ClickHouse中的最小数据单元，这里在/blocks节点中写入block_id主要是为了后续数据的去重\n接着向/log节点推送日志，日志信息如下所示：\nformat version: 4 create_time: 2022-09-04 14:30:58 source replica: chi-0 block_id: 20220904_5211346952104599192_1472622755444261990 get 20220904_269677_269677_0 part_type: Compact ... 日志内容中主要包含了源节点chi-0，操作类型get，分区目录20220904_269677_269677_0，以及block_id。\n接下来就轮到副本节点chi-1的回合了，由于chi-1也监听了/log节点，通过分析日志信息，它需要执行获取part的操作。因此它获取到日志信息后，会将其推送到/replicas/chi-1/queue/节点下，稍后在执行。\n把日志推送到队列中，而不是立马执行。主要是处于性能的考虑，比如同一时间段可能会收到很多日志信息，可以需要将其攒批处理以提升性能\n随后，chi-1节点从队列中获取任务开始执行，其首先从/replicas获取所有副本，选择一个合适的副本(chi-0)，对其发起数据下载的请求\n选择一个合适的副本，往往需要考虑副本拥有数据的新旧程度以及副本节点的负载\nchi-0收到下载请求后，发送相关part的数据给chi-1，chi-1完成写入\n合并 合并操作本质上也是由各个副本独立完成的，不会涉及到任何part数据的传输。首先，合并操作可以在任何节点上触发，但是都必须由主节点来发布合并任务。\n假设，在从节点chi-1上，我们触发了合并请求（可通过OPTIMIZE操作触发） 此时，chi-1不会立马执行合并操作，而是向主节点chi-0发送请求，并由chi-0来指定合并计划 chi-0将合并计划生成操作日志推送到/log下 所有副本(包括主副本)监听到操作日志后，将合并任务推送到各自的/queue下 副本各自监听/queue，收到合并任务后，各自分别执行合并任务 查询 当客户端发起查询请求时，由于设计多个副本，因此需要考虑负载均衡的问题。对此，ClickHouse会根据配置负载均衡算法来选择一个合适的副本，由load_babalancing参数决定。\n分片机制 为解决可扩展性的问题，ClickHouse引入了分片机制，主要通过Distributed引擎来定义，DDL如下所示：\nCREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE = Distributed(cluster, database, table[, sharding_key[, policy_name]]) [SETTINGS name=value, ...] 参数说明如下所示：\ncluster：表示集群名 database：表示所管辖的本地表的数据库名 table：表示所管辖的本地表的表明 sharding_key：表示分片键， 通过Distributed定义的分布式表，同样也是虚拟表，实际数据由其管理的本地表存储，如图所示：\n分片规则 分片规则主要是用以路由数据，决定数据存储在哪个分片中，规则主要由sharding_key和weight参数决定：\nsharding_key：用来生成分片的key\nweight：表示分片权重，通过服务端的配置来设置。其可以用来调整分片之间数据的分布，默认都是1。若某个分片的weight越大，则数据会向该分片倾斜。\n假定只有0号和1号两个分片，其中weight分别为10和20，sharding_key为rand()（生成随机整数），则最终的表达式为如下所示:\nshard_number = rand() % 30\n当shard_number位于[0, 10)之间时，数据被路由到0号分片\n当shard_number位于[10, 30)之间时，数据则被路由到1号分片\n写入 通过上述的分片规则，我们就可以确定写入请求要被写入到那个节点上了，此时再对该节点发起请求。这种方式是非常高效也是官方的推荐方式，因为其直接在客户端决定了数据流向，无需额外的路由操作，不过缺点就是客户端实现比较复杂。\n除此之外，我们还可以直接往Distributed表里插入数据，此时数据的分片和路由则由ClickHouse节点代理了，当数据不属于当前节点上的分片时，则需要将数据发送到目标分片所在的节点上，从而会导致额外的网络传输，影响性能。\n查询 与写入不同的是，分布式查询是直接查询Distributed表。\n节点收到查询请求后，节点将分布式查询转换为本地表查询，并将查询下发到各个分片上 各个分片各自执行本地表查询，然后将查询结果返回节点 最后，节点将结果集合并成最终结果，返回给客户端 我们通过EXPLAIN语句可以看到，分布式查询会分别从两个分片中获取数据并合并\n小心子查询 当涉及到子查询时，需要特别小心，因为很容易导致结果错误或者性能下降。\n这里，我们假定一个4节点的集群，users_all和orders_all分别为本地表users和orders的分布式表，单副本双分片\n结果错误 select username from users_all where id IN (select distinct(user_id) from orders) ch-0首先将该SQL转换成本地SQL：select username from users where id IN (select distinct(user_id) from orders)，然后下发本地SQL到users的分片节点上执行 分片节点得到子结果后，将结果返回给ch-0，ch-0合并并生成最终结果。 看起来没问题？其实并非如此，此时得到的结果是不正确的。\n假若orders表的分片在ch-2和ch-3节点上，而orders表的分片在ch-0和ch-1上，则子查询的结果均为空，因此最终结果也为空 假若orders表的分片在ch-2和ch-3节点上，而orders表的分片在ch-0和ch-2上，则两个子查询中的只有ch-2节点上的子查询能得到结果，因此最终结果仅为正确结果的一部分 这是因为子查询为本地表查询，若节点上不包含orders的分片数据，那么就只会得到空数据，因此子查询语句同样需要改为分布式查询\n性能下降 此时，SQL则会变成\nselect username from users_all where id IN (select distinct(user_id) from orders_all) ch-0首先将该SQL转换成本地SQL：select username from users where id IN (select distinct(user_id) from orders_all)，然后下发到users所在的分片节点上执行\n分片节点再将其子查询转换为本地SQL：select distinct(user_id) from orders下发到orders所在的分片节点上执行，得到结果集\norders所在的分片节点分别执行分布式查询得到结果集，再执行原SQL得到子结果集，返回给ch-0，ch-0合并并生成最终结果。\n可以明显看到，子查询在分片ch-2和ch-3上分别执行了分布式查询（ch-2下发子查询的本地SQL到ch-3，而ch-3下发子查询的本地SQL到ch-2），同样的结果集被查询了两次，从而导致查询性能下降，且orders的分片数越多，性能下降越明显。\n幸运的是，ClickHouse提供了GOBAL IN语句来解决此类子查询问题，此时ClickHouse会先单独执行子查询，得到的结果存在一个临时内存表里，并将内存表的数据发送到users的分片节点上。\n总结 在本文中，我们分别介绍了ClickHouse是如何利用副本机制用以解决高可用问题场景，以及利用分片机制用以解决可扩展性问题。\n总的来说，ClickHouse的采用了多主架构，避免了主从架构的单点问题，将负载均摊到各个节点，并且给予用户足够的灵活度，例如可以将本地表转换为分布式表、将分布式表转换成本地表、调整权重控制数据倾斜度等等。\n然而，其缺点也比较明显，一则配置比较繁琐，需要较多的人工运维操作；二则，严重依赖zookeeper，基本所有操作都需要zookeeper来进行信息的同步与交换，当存在很多表时会对zookeeper产生较大压力，从而影响整体集群的性能。\n参考 https://clickhouse.com/docs/en/development/architecture/#replication https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replication https://clickhouse.com/docs/en/engines/table-engines/special/distributed ","wordCount":"221","inLanguage":"zh","datePublished":"2023-04-10T16:24:04+08:00","dateModified":"2023-04-10T16:24:04+08:00","author":{"@type":"Person","name":"erenming"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://erenming.github.io/posts/clickhouse-distrubuted-arch/"},"publisher":{"@type":"Organization","name":"Nothing Special","logo":{"@type":"ImageObject","url":"https://erenming.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://erenming.github.io/ accesskey=h title="Nothing Special (Alt + H)">Nothing Special</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://erenming.github.io/en/ title=English aria-label=English>En</a></li></ul></div></div><ul id=menu><li><a href=https://erenming.github.io/search/ title="搜索 (Alt + /)" accesskey=/><span>搜索</span></a></li><li><a href=https://erenming.github.io/archives/ title=归档><span>归档</span></a></li><li><a href=https://erenming.github.io/tags/ title=标签><span>标签</span></a></li><li><a href=https://erenming.github.io/readings/ title=书单><span>书单</span></a></li><li><a href=https://erenming.github.io/about/ title=关于><span>关于</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">ClickHouse的分布式实现</h1><div class=post-meta><span title='2023-04-10 16:24:04 +0800 +0800'>四月 10, 2023</span>&nbsp;·&nbsp;2 分钟&nbsp;·&nbsp;erenming</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>目录</span></summary><div class=inner><ul><li><a href=#%e5%89%af%e6%9c%ac%e6%9c%ba%e5%88%b6 aria-label=副本机制>副本机制</a><ul><li><a href=#%e5%bb%ba%e8%a1%a8 aria-label=建表>建表</a></li><li><a href=#%e5%86%99%e5%85%a5 aria-label=写入>写入</a></li><li><a href=#%e5%90%88%e5%b9%b6 aria-label=合并>合并</a></li><li><a href=#%e6%9f%a5%e8%af%a2 aria-label=查询>查询</a></li></ul></li><li><a href=#%e5%88%86%e7%89%87%e6%9c%ba%e5%88%b6 aria-label=分片机制>分片机制</a><ul><li><a href=#%e5%88%86%e7%89%87%e8%a7%84%e5%88%99 aria-label=分片规则>分片规则</a></li><li><a href=#%e5%86%99%e5%85%a5-1 aria-label=写入>写入</a></li><li><a href=#%e6%9f%a5%e8%af%a2-1 aria-label=查询>查询</a><ul><li><a href=#%e5%b0%8f%e5%bf%83%e5%ad%90%e6%9f%a5%e8%af%a2 aria-label=小心子查询>小心子查询</a><ul><li><a href=#%e7%bb%93%e6%9e%9c%e9%94%99%e8%af%af aria-label=结果错误>结果错误</a></li><li><a href=#%e6%80%a7%e8%83%bd%e4%b8%8b%e9%99%8d aria-label=性能下降>性能下降</a></li></ul></li></ul></li></ul></li><li><a href=#%e6%80%bb%e7%bb%93 aria-label=总结>总结</a></li><li><a href=#%e5%8f%82%e8%80%83 aria-label=参考>参考</a></li></ul></div></details></div><div class=post-content><p>当我们需要在实际生产环境中使用ClickHouse时，高可用与可扩展是绕不开的话题，因此ClickHouse也提供了分布式的相关机制来应对这些问题。在下文中，我们将主要从副本机制、分片机制两个个方面来对齐进行介绍。</p><h1 id=副本机制>副本机制<a hidden class=anchor aria-hidden=true href=#副本机制>#</a></h1><p>ClickHouse通过扩展<code>MergeTree</code>为<code>ReplicatedMergeTree</code>来创建副本表引擎（通过在MergeTree添加Replicated前缀来表示副本表引擎）。这里需要注意的是，副本表并非一种具体的表引擎，而是一种逻辑上的表引擎，实际数据的存取仍然通过<code>MergeTree</code>来完成。</p><blockquote><p>注意：这里，我们假定集群名为local，且包含两个节点chi-0和chi-1</p></blockquote><h2 id=建表>建表<a hidden class=anchor aria-hidden=true href=#建表>#</a></h2><p>ReplicatedMergeTree通过类似如下语句进行创建：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CREATE</span> <span style=color:#66d9ef>TABLE</span> <span style=color:#66d9ef>table_name</span>
</span></span><span style=display:flex><span>(
</span></span><span style=display:flex><span>    EventDate DateTime,
</span></span><span style=display:flex><span>    CounterID UInt32,
</span></span><span style=display:flex><span>    UserID UInt32,
</span></span><span style=display:flex><span>    ver UInt16
</span></span><span style=display:flex><span>) ENGINE <span style=color:#f92672>=</span> ReplicatedReplacingMergeTree(<span style=color:#e6db74>&#39;/clickhouse/tables/{cluster}-{shard}/table_name&#39;</span>, <span style=color:#e6db74>&#39;{replica}&#39;</span>, ver)
</span></span><span style=display:flex><span>PARTITION <span style=color:#66d9ef>BY</span> toYYYYMM(EventDate)
</span></span><span style=display:flex><span><span style=color:#66d9ef>ORDER</span> <span style=color:#66d9ef>BY</span> (CounterID, EventDate, intHash32(UserID))
</span></span><span style=display:flex><span>SAMPLE <span style=color:#66d9ef>BY</span> intHash32(UserID);
</span></span></code></pre></div><p>有两个参数需要重点说明一下，分别为zoo_path和replica_name参数：</p><ul><li>zoo_path: 表示表所在的zk路径</li><li>replica_name: 表示副本名称，通常为主机名</li></ul><blockquote><p>ClickHouse会在zk中建立路径zoo_path，并在zoo_path的子目录/replicas下根据replica_name创建副本标识，因此可以看到replica_name参数的作用主要就是用来作为副本ID。</p></blockquote><p>我们这里假定首先在chi-0节点上执行了建表语句</p><ul><li>其首先创建一个副本实例，进行一些初始化的工作，在zk上创建相关节点</li><li>接着在/replicas节点下注册副本实例<strong>chi-0</strong></li><li>启用监听任务，监听/log节点</li><li>参与leader节点选举(通过向/leader_election写入数据，谁先写入成功谁就是leader)</li></ul><p>接着，我们在chi-1节点上执行建表语句：</p><ul><li>首先也是创建副本实例，进行初始化工作</li><li>接着在/replicas节点下注册副本实例<strong>chi-1</strong></li><li>启用监听任务，监听/log节点</li><li>参与leader节点选举(此时由于chi-0节点上已经执行过建表流程了，因此chi-0为leader副本)</li></ul><blockquote><p>/log节点非常重要，用来记录各种操作LogEntry包括获取part，合并part，删除分区等等操作</p></blockquote><h2 id=写入>写入<a hidden class=anchor aria-hidden=true href=#写入>#</a></h2><p>接着，我们通过执行<code>INSERT INTO</code>语句向chi-0节点写入数据（当写入请求被发到从节点时，从节点会将其转发到主节点）。</p><p>此时，会首先在本地完成分区数据的写入，然后向/blocks节点写入该分区的block_id</p><blockquote><p>block是ClickHouse中的最小数据单元，这里在/blocks节点中写入block_id主要是为了后续数据的去重</p></blockquote><p>接着向/log节点推送日志，日志信息如下所示：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>format version: <span style=color:#ae81ff>4</span>
</span></span><span style=display:flex><span>create_time: 2022-09-04 14:30:58
</span></span><span style=display:flex><span>source replica: chi-0
</span></span><span style=display:flex><span>block_id: 20220904_5211346952104599192_1472622755444261990
</span></span><span style=display:flex><span>get
</span></span><span style=display:flex><span>20220904_269677_269677_0
</span></span><span style=display:flex><span>part_type: Compact
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>日志内容中主要包含了源节点chi-0，操作类型get，分区目录20220904_269677_269677_0，以及block_id。</p><p>接下来就轮到副本节点chi-1的回合了，由于chi-1也监听了/log节点，通过分析日志信息，它需要执行获取part的操作。因此它获取到日志信息后，会将其推送到/replicas/chi-1/queue/节点下，稍后在执行。</p><blockquote><p>把日志推送到队列中，而不是立马执行。主要是处于性能的考虑，比如同一时间段可能会收到很多日志信息，可以需要将其攒批处理以提升性能</p></blockquote><p>随后，chi-1节点从队列中获取任务开始执行，其首先从/replicas获取所有副本，选择一个合适的副本(chi-0)，对其发起数据下载的请求</p><blockquote><p>选择一个合适的副本，往往需要考虑副本拥有数据的新旧程度以及副本节点的负载</p></blockquote><p>chi-0收到下载请求后，发送相关part的数据给chi-1，chi-1完成写入</p><h2 id=合并>合并<a hidden class=anchor aria-hidden=true href=#合并>#</a></h2><p>合并操作本质上也是由各个副本独立完成的，不会涉及到任何part数据的传输。首先，合并操作可以在任何节点上触发，但是都必须由主节点来发布合并任务。</p><ol><li>假设，在从节点chi-1上，我们触发了合并请求（可通过OPTIMIZE操作触发）</li><li>此时，chi-1不会立马执行合并操作，而是向主节点chi-0发送请求，并由chi-0来指定合并计划</li><li>chi-0将合并计划生成操作日志推送到/log下</li><li>所有副本(包括主副本)监听到操作日志后，将合并任务推送到各自的/queue下</li><li>副本各自监听/queue，收到合并任务后，各自分别执行合并任务</li></ol><h2 id=查询>查询<a hidden class=anchor aria-hidden=true href=#查询>#</a></h2><p>当客户端发起查询请求时，由于设计多个副本，因此需要考虑负载均衡的问题。对此，ClickHouse会根据配置负载均衡算法来选择一个合适的副本，由<a href=https://clickhouse.com/docs/en/operations/settings/settings/#settings-load_balancing>load_babalancing</a>参数决定。</p><h1 id=分片机制>分片机制<a hidden class=anchor aria-hidden=true href=#分片机制>#</a></h1><p>为解决可扩展性的问题，ClickHouse引入了分片机制，主要通过Distributed引擎来定义，DDL如下所示：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>CREATE</span> <span style=color:#66d9ef>TABLE</span> [<span style=color:#66d9ef>IF</span> <span style=color:#66d9ef>NOT</span> <span style=color:#66d9ef>EXISTS</span>] [db.]<span style=color:#66d9ef>table_name</span> [<span style=color:#66d9ef>ON</span> <span style=color:#66d9ef>CLUSTER</span> <span style=color:#66d9ef>cluster</span>]
</span></span><span style=display:flex><span>(
</span></span><span style=display:flex><span>    name1 [type1] [<span style=color:#66d9ef>DEFAULT</span><span style=color:#f92672>|</span>MATERIALIZED<span style=color:#f92672>|</span><span style=color:#66d9ef>ALIAS</span> expr1],
</span></span><span style=display:flex><span>    name2 [type2] [<span style=color:#66d9ef>DEFAULT</span><span style=color:#f92672>|</span>MATERIALIZED<span style=color:#f92672>|</span><span style=color:#66d9ef>ALIAS</span> expr2],
</span></span><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>) ENGINE <span style=color:#f92672>=</span> Distributed(<span style=color:#66d9ef>cluster</span>, <span style=color:#66d9ef>database</span>, <span style=color:#66d9ef>table</span>[, sharding_key[, policy_name]])
</span></span><span style=display:flex><span>[SETTINGS name<span style=color:#f92672>=</span>value, ...]
</span></span></code></pre></div><p>参数说明如下所示：</p><ul><li><code>cluster</code>：表示集群名</li><li><code>database</code>：表示所管辖的本地表的数据库名</li><li><code>table</code>：表示所管辖的本地表的表明</li><li><code>sharding_key</code>：表示分片键，</li></ul><p>通过Distributed定义的分布式表，同样也是虚拟表，实际数据由其管理的本地表存储，如图所示：</p><p><img loading=lazy src=https://raw.githubusercontent.com/erenming/image-pool/master/blog/image-20220908223608229.png alt=image-20220908223608229></p><h2 id=分片规则>分片规则<a hidden class=anchor aria-hidden=true href=#分片规则>#</a></h2><p>分片规则主要是用以路由数据，决定数据存储在哪个分片中，规则主要由sharding_key和weight参数决定：</p><ul><li><p><strong>sharding_key</strong>：用来生成分片的key</p></li><li><p><strong>weight</strong>：表示分片权重，通过服务端的配置来设置。其可以用来调整分片之间数据的分布，默认都是1。若某个分片的weight越大，则数据会向该分片倾斜。</p></li></ul><p>假定只有0号和1号两个分片，其中weight分别为10和20，sharding_key为rand()（生成随机整数），则最终的表达式为如下所示:</p><blockquote><p>shard_number = rand() % 30</p><p>当shard_number位于[0, 10)之间时，数据被路由到0号分片</p><p>当shard_number位于[10, 30)之间时，数据则被路由到1号分片</p></blockquote><h2 id=写入-1>写入<a hidden class=anchor aria-hidden=true href=#写入-1>#</a></h2><p>通过上述的分片规则，我们就可以确定写入请求要被写入到那个节点上了，此时再对该节点发起请求。这种方式是非常高效也是官方的推荐方式，因为其直接在客户端决定了数据流向，无需额外的路由操作，不过缺点就是客户端实现比较复杂。</p><p>除此之外，我们还可以直接往Distributed表里插入数据，此时数据的分片和路由则由ClickHouse节点代理了，当数据不属于当前节点上的分片时，则需要将数据发送到目标分片所在的节点上，从而会导致额外的网络传输，影响性能。</p><h2 id=查询-1>查询<a hidden class=anchor aria-hidden=true href=#查询-1>#</a></h2><p>与写入不同的是，分布式查询是直接查询Distributed表。</p><ol><li>节点收到查询请求后，节点将分布式查询转换为本地表查询，并将查询下发到各个分片上</li><li>各个分片各自执行本地表查询，然后将查询结果返回节点</li><li>最后，节点将结果集合并成最终结果，返回给客户端</li></ol><p>我们通过EXPLAIN语句可以看到，分布式查询会分别从两个分片中获取数据并合并</p><p><img loading=lazy src=https://raw.githubusercontent.com/erenming/image-pool/master/blog/image-20220908232053946.png alt=image-20220908232053946></p><h3 id=小心子查询>小心子查询<a hidden class=anchor aria-hidden=true href=#小心子查询>#</a></h3><p>当涉及到子查询时，需要特别小心，因为很容易导致结果错误或者性能下降。</p><blockquote><p>这里，我们假定一个4节点的集群，users_all和orders_all分别为本地表users和orders的分布式表，单副本双分片</p></blockquote><h4 id=结果错误>结果错误<a hidden class=anchor aria-hidden=true href=#结果错误>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sql data-lang=sql><span style=display:flex><span><span style=color:#66d9ef>select</span> username <span style=color:#66d9ef>from</span> users_all <span style=color:#66d9ef>where</span> id <span style=color:#66d9ef>IN</span> (<span style=color:#66d9ef>select</span> <span style=color:#66d9ef>distinct</span>(user_id) <span style=color:#66d9ef>from</span> orders)
</span></span></code></pre></div><ul><li>ch-0首先将该SQL转换成本地SQL：<code>select username from users where id IN (select distinct(user_id) from orders)</code>，然后下发本地SQL到users的分片节点上执行</li><li>分片节点得到子结果后，将结果返回给ch-0，ch-0合并并生成最终结果。</li></ul><p>看起来没问题？其实并非如此，此时得到的结果是不正确的。</p><ul><li>假若orders表的分片在ch-2和ch-3节点上，而orders表的分片在ch-0和ch-1上，则子查询的结果均为空，因此最终结果也为空</li></ul><p><img loading=lazy src=https://raw.githubusercontent.com/erenming/image-pool/master/blog/image-20220912111324196.png alt=image-20220912111324196></p><ul><li>假若orders表的分片在ch-2和ch-3节点上，而orders表的分片在ch-0和ch-2上，则两个子查询中的只有ch-2节点上的子查询能得到结果，因此最终结果仅为正确结果的一部分</li></ul><p><img loading=lazy src=https://raw.githubusercontent.com/erenming/image-pool/master/blog/image-20220912111455101.png alt=image-20220912111455101></p><p>这是因为子查询为本地表查询，若节点上不包含orders的分片数据，那么就只会得到空数据，因此子查询语句同样需要改为分布式查询</p><h4 id=性能下降>性能下降<a hidden class=anchor aria-hidden=true href=#性能下降>#</a></h4><p>此时，SQL则会变成</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-SQL data-lang=SQL><span style=display:flex><span><span style=color:#66d9ef>select</span> username <span style=color:#66d9ef>from</span> users_all <span style=color:#66d9ef>where</span> id <span style=color:#66d9ef>IN</span> (<span style=color:#66d9ef>select</span> <span style=color:#66d9ef>distinct</span>(user_id) <span style=color:#66d9ef>from</span> orders_all)
</span></span></code></pre></div><ul><li><p>ch-0首先将该SQL转换成本地SQL：<code>select username from users where id IN (select distinct(user_id) from orders_all)</code>，然后下发到users所在的分片节点上执行</p></li><li><p>分片节点再将其子查询转换为本地SQL：<code>select distinct(user_id) from orders</code>下发到orders所在的分片节点上执行，得到结果集</p></li><li><p>orders所在的分片节点分别执行分布式查询得到结果集，再执行原SQL得到子结果集，返回给ch-0，ch-0合并并生成最终结果。</p></li></ul><p><img loading=lazy src=https://raw.githubusercontent.com/erenming/image-pool/master/blog/image-20220912120811422.png alt=image-20220912120811422></p><p>可以明显看到，子查询在分片ch-2和ch-3上分别执行了分布式查询（ch-2下发子查询的本地SQL到ch-3，而ch-3下发子查询的本地SQL到ch-2），同样的结果集被查询了两次，从而导致查询性能下降，且orders的分片数越多，性能下降越明显。</p><p>幸运的是，ClickHouse提供了<a href=https://clickhouse.com/docs/en/sql-reference/operators/in#select-in-operators>GOBAL IN</a>语句来解决此类子查询问题，此时ClickHouse会先单独执行子查询，得到的结果存在一个临时内存表里，并将内存表的数据发送到users的分片节点上。</p><h1 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h1><p>在本文中，我们分别介绍了ClickHouse是如何利用副本机制用以解决高可用问题场景，以及利用分片机制用以解决可扩展性问题。</p><p>总的来说，ClickHouse的采用了多主架构，避免了主从架构的单点问题，将负载均摊到各个节点，并且给予用户足够的灵活度，例如可以将本地表转换为分布式表、将分布式表转换成本地表、调整权重控制数据倾斜度等等。</p><p>然而，其缺点也比较明显，一则配置比较繁琐，需要较多的人工运维操作；二则，严重依赖zookeeper，基本所有操作都需要zookeeper来进行信息的同步与交换，当存在很多表时会对zookeeper产生较大压力，从而影响整体集群的性能。</p><h1 id=参考>参考<a hidden class=anchor aria-hidden=true href=#参考>#</a></h1><ul><li><a href=https://clickhouse.com/docs/en/development/architecture/#replication>https://clickhouse.com/docs/en/development/architecture/#replication</a></li><li><a href=https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replication>https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replication</a></li><li><a href=https://clickhouse.com/docs/en/engines/table-engines/special/distributed>https://clickhouse.com/docs/en/engines/table-engines/special/distributed</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://erenming.github.io/tags/clickhouse/>ClickHouse</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://erenming.github.io/>Nothing Special</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>